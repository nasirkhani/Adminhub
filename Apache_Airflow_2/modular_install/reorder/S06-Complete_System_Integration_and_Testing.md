# S06-Complete_System_Integration_and_Testing.md

## Comprehensive Testing and Validation of Distributed HA Infrastructure

### Step 6.1: System Integration Verification

**‚ö†Ô∏è IMPORTANT:  Replace all IP placeholders with your actual VM IP addresses before running**

**Verify complete infrastructure status:**
```bash
# Create comprehensive system status script
sudo tee /usr/local/bin/airflow-ha-status.sh << 'EOF'
#!/bin/bash

echo "=================================="
echo "  AIRFLOW HA INFRASTRUCTURE STATUS"
echo "=================================="
echo "Timestamp: $(date)"
echo ""

# VIP Status - REPLACE_WITH_YOUR_VIPs
echo "=== VIRTUAL IPs ==="
ping -c 1 <MAIN_VIP> >/dev/null 2>&1 && echo "‚úÖ Main VIP (<MAIN_VIP>): ACCESSIBLE" || echo "‚ùå Main VIP: FAILED"
ping -c 1 <NFS_VIP> >/dev/null 2>&1 && echo "‚úÖ NFS VIP (<NFS_VIP>): ACCESSIBLE" || echo "‚ùå NFS VIP: FAILED"
echo ""

# Database Cluster Status
echo "=== DATABASE CLUSTER ==="
export PGPASSWORD=airflow_pass
if psql -h <MAIN_VIP> -U airflow_user -p 5000 -d airflow_db -c "SELECT 1;" >/dev/null 2>&1; then
    echo "‚úÖ PostgreSQL via VIP: ACCESSIBLE"
    LEADER=$(psql -h <MAIN_VIP> -U airflow_user -p 5000 -d airflow_db -t -c "SELECT inet_server_addr();" 2>/dev/null | xargs)
    echo "   Current Leader: $LEADER"
else
    echo "‚ùå PostgreSQL via VIP: FAILED"
fi
echo ""

# RabbitMQ Cluster Status
echo "=== RABBITMQ CLUSTER ==="
HEALTHY_MQ=0
# REPLACE_WITH_YOUR_RABBIT_IPs - Update these IP addresses
for node in <RABBIT_1_IP> <RABBIT_2_IP> <RABBIT_3_IP>; do
    if curl -s -u airflow_user:airflow_pass http://$node:15672/api/overview >/dev/null 2>&1; then
        echo "‚úÖ RabbitMQ $node: HEALTHY"
        HEALTHY_MQ=$((HEALTHY_MQ + 1))
    else
        echo "‚ùå RabbitMQ $node: DOWN"
    fi
done
echo "   Healthy nodes: $HEALTHY_MQ/3"
echo ""

# HAProxy Status
echo "=== HAPROXY CLUSTER ==="
if ip addr show | grep -q <MAIN_VIP>; then
    echo "‚úÖ HAProxy Primary (haproxy-1): ACTIVE (has VIP)"
    systemctl is-active --quiet haproxy && echo "   HAProxy service: RUNNING" || echo "   HAProxy service: FAILED"
elif ssh -o ConnectTimeout=5 rocky@haproxy-2 "ip addr show | grep -q <MAIN_VIP>" 2>/dev/null; then
    echo "‚úÖ HAProxy Standby (haproxy-2): ACTIVE (has VIP)"
    echo "   Failover detected"
else
    echo "‚ùå HAProxy: VIP NOT FOUND"
fi
echo ""

# NFS Storage Status
echo "=== NFS STORAGE ==="
if ip addr show | grep -q <NFS_VIP>; then
    echo "‚úÖ NFS Primary (nfs-1): ACTIVE (has VIP)"
    systemctl is-active --quiet nfs-server && echo "   NFS service: RUNNING" || echo "   NFS service: FAILED"
elif ssh -o ConnectTimeout=5 rocky@nfs-2 "ip addr show | grep -q <NFS_VIP>" 2>/dev/null; then
    echo "‚úÖ NFS Standby (nfs-2): ACTIVE (has VIP)"
    echo "   Failover detected"
else
    echo "‚ùå NFS: VIP NOT FOUND"
fi

# Test NFS access from clients
for vm in "haproxy-1" "haproxy-2" "scheduler-2" "celery-1"; do
    if [ "$vm" = "haproxy-1" ]; then
        ls /mnt/airflow-dags >/dev/null 2>&1 && echo "   $vm NFS access: OK" || echo "   $vm NFS access: FAILED"
    else
        ssh -o ConnectTimeout=5 rocky@$vm "ls /mnt/airflow-dags" >/dev/null 2>&1 && echo "   $vm NFS access: OK" || echo "   $vm NFS access: FAILED"
    fi
done
echo ""

# Airflow Services Status
echo "=== AIRFLOW SERVICES ==="
echo "Schedulers:"
systemctl is-active --quiet airflow-scheduler && echo "   ‚úÖ haproxy-1 Scheduler: RUNNING" || echo "   ‚ùå haproxy-1 Scheduler: FAILED"
ssh -o ConnectTimeout=5 rocky@scheduler-2 "systemctl is-active --quiet airflow-scheduler" 2>/dev/null && echo "   ‚úÖ scheduler-2 Scheduler: RUNNING" || echo "   ‚ùå scheduler-2 Scheduler: FAILED"

echo "Webservers:"
systemctl is-active --quiet airflow-webserver && echo "   ‚úÖ haproxy-1 Webserver: RUNNING" || echo "   ‚ùå haproxy-1 Webserver: FAILED"
ssh -o ConnectTimeout=5 rocky@haproxy-2 "systemctl is-active --quiet airflow-webserver" 2>/dev/null && echo "   ‚úÖ haproxy-2 Webserver: RUNNING" || echo "   ‚ùå haproxy-2 Webserver: FAILED"

echo "Workers:"
ssh -o ConnectTimeout=5 rocky@celery-1 "systemctl is-active --quiet airflow-worker" 2>/dev/null && echo "   ‚úÖ celery-1 Worker: RUNNING" || echo "   ‚ùå celery-1 Worker: FAILED"

echo "DAG Processors:"
systemctl is-active --quiet airflow-dag-processor && echo "   ‚úÖ nfs-1 DAG Processor: RUNNING" || echo "   ‚ùå nfs-1 DAG Processor: STOPPED"
ssh -o ConnectTimeout=5 rocky@nfs-2 "systemctl is-active --quiet airflow-dag-processor" 2>/dev/null && echo "   ‚úÖ nfs-2 DAG Processor: RUNNING" || echo "   ‚ùå nfs-2 DAG Processor: STOPPED"

echo "Monitoring:"
systemctl is-active --quiet airflow-flower && echo "   ‚úÖ Flower: RUNNING" || echo "   ‚ùå Flower: FAILED"
echo ""

# Access URLs - REPLACE_WITH_YOUR_VIPs_AND_IPs
echo "=== ACCESS URLS ==="
echo "Main Airflow UI: http://<MAIN_VIP>:8081"
echo "Flower Monitor:  http://<HAPROXY_1_IP>:5555"
echo "HAProxy Stats:   http://<MAIN_VIP>:7000"
echo "RabbitMQ Mgmt:   http://<RABBIT_1_IP>:15672"
echo ""

echo "=================================="
EOF

sudo chmod +x /usr/local/bin/airflow-ha-status.sh
```

**üîß Status Script IP Replacement Helper:**
```bash
# Replace IP placeholders in status script
# CUSTOMIZE these values with your actual addresses:
MAIN_VIP="192.168.1.210"         # Replace with your Main VIP
NFS_VIP="192.168.1.220"          # Replace with your NFS VIP
HAPROXY_1_IP="192.168.1.10"      # Replace with haproxy-1 IP
RABBIT_1_IP="192.168.1.15"       # Replace with rabbit-1 IP
RABBIT_2_IP="192.168.1.16"       # Replace with rabbit-2 IP
RABBIT_3_IP="192.168.1.17"       # Replace with rabbit-3 IP

# Apply replacements to status script
sudo sed -i "s/<MAIN_VIP>/$MAIN_VIP/g" /usr/local/bin/airflow-ha-status.sh
sudo sed -i "s/<NFS_VIP>/$NFS_VIP/g" /usr/local/bin/airflow-ha-status.sh
sudo sed -i "s/<HAPROXY_1_IP>/$HAPROXY_1_IP/g" /usr/local/bin/airflow-ha-status.sh
sudo sed -i "s/<RABBIT_1_IP>/$RABBIT_1_IP/g" /usr/local/bin/airflow-ha-status.sh
sudo sed -i "s/<RABBIT_2_IP>/$RABBIT_2_IP/g" /usr/local/bin/airflow-ha-status.sh
sudo sed -i "s/<RABBIT_3_IP>/$RABBIT_3_IP/g" /usr/local/bin/airflow-ha-status.sh

echo "Status script updated with your IP addresses"

# Run initial status check
sudo /usr/local/bin/airflow-ha-status.sh
```

### Step 6.2: Comprehensive Functionality Testing

**Test complete DAG lifecycle:**
```bash
# Test Airflow CLI functionality
echo "=== AIRFLOW CLI TESTS ==="
airflow db check && echo "‚úÖ Database connectivity: OK"
airflow dags list | head -3 && echo "‚úÖ DAG listing: OK"

# Test user authentication
airflow users list | grep -E "(admin|operator|viewer)" && echo "‚úÖ User management: OK"

# Test DAG operations
if airflow dags list | grep -q test_distributed_ha_setup; then
    echo "‚úÖ Test DAG parsed: OK"
    
    # Unpause and trigger test DAG
    airflow dags unpause test_distributed_ha_setup
    airflow dags trigger test_distributed_ha_setup
    
    echo "Test DAG triggered - waiting for execution..."
    sleep 60
    
    # Check execution status
    airflow dags state test_distributed_ha_setup $(date +%Y-%m-%d) && echo "‚úÖ DAG execution: COMPLETED"
    
    # Get task execution details
    airflow tasks states-for-dag-run test_distributed_ha_setup $(date +%Y-%m-%d)
else
    echo "‚ùå Test DAG not found"
fi
```

### Step 6.3: High Availability Failover Testing

**‚ö†Ô∏è IMPORTANT: Replace `<POSTGRESQL_*_IP>` and `<MAIN_VIP>` with your actual addresses**

**Test Database HA Failover:**
```bash
echo "=== DATABASE HA FAILOVER TEST ==="

# CUSTOMIZE this value:
MAIN_VIP="192.168.1.210"  # Replace with your Main VIP

# Get current database leader
export PGPASSWORD=airflow_pass
CURRENT_LEADER=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT inet_server_addr();" 2>/dev/null | xargs)
echo "Current DB Leader: $CURRENT_LEADER"

# Stop current leader to force failover - REPLACE_WITH_YOUR_POSTGRESQL_IPs
# CUSTOMIZE these values:
POSTGRESQL_1_IP="192.168.1.18"  # Replace with postgresql-1 IP
POSTGRESQL_2_IP="192.168.1.19"  # Replace with postgresql-2 IP
POSTGRESQL_3_IP="192.168.1.20"  # Replace with postgresql-3 IP

case $CURRENT_LEADER in
    "$POSTGRESQL_1_IP")
        echo "Stopping postgresql-1 to test failover..."
        ssh rocky@postgresql-1 "sudo systemctl stop patroni"
        ;;
    "$POSTGRESQL_2_IP")
        echo "Stopping postgresql-2 to test failover..."
        ssh rocky@postgresql-2 "sudo systemctl stop patroni"
        ;;
    "$POSTGRESQL_3_IP")
        echo "Stopping postgresql-3 to test failover..."
        ssh rocky@postgresql-3 "sudo systemctl stop patroni"
        ;;
esac

# Wait for failover
echo "Waiting for database failover..."
sleep 30

# Test database accessibility after failover
if psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -c "SELECT 'Database failover: SUCCESS';" 2>/dev/null; then
    NEW_LEADER=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT inet_server_addr();" 2>/dev/null | xargs)
    echo "‚úÖ Database failover successful"
    echo "New Leader: $NEW_LEADER"
else
    echo "‚ùå Database failover failed"
fi

# Restart the stopped node
case $CURRENT_LEADER in
    "$POSTGRESQL_1_IP")
        ssh rocky@postgresql-1 "sudo systemctl start patroni"
        ;;
    "$POSTGRESQL_2_IP")
        ssh rocky@postgresql-2 "sudo systemctl start patroni"
        ;;
    "$POSTGRESQL_3_IP")
        ssh rocky@postgresql-3 "sudo systemctl start patroni"
        ;;
esac

echo "Database failover test completed"
echo ""
```

**Test HAProxy HA Failover:**
```bash
echo "=== HAPROXY HA FAILOVER TEST ==="

# CUSTOMIZE this value:
MAIN_VIP="192.168.1.210"  # Replace with your Main VIP

# Check current HAProxy primary
if ip addr show | grep -q $MAIN_VIP; then
    echo "haproxy-1 is current HAProxy primary"
    PRIMARY="haproxy-1"
else
    echo "haproxy-2 is current HAProxy primary"
    PRIMARY="haproxy-2"
fi

# Test webserver access before failover
curl -s -I http://$MAIN_VIP:8081 | head -1 && echo "Webserver accessible before failover"

# Stop HAProxy on primary to trigger failover
if [ "$PRIMARY" = "haproxy-1" ]; then
    echo "Stopping HAProxy on haproxy-1..."
    sudo systemctl stop haproxy
else
    echo "Stopping HAProxy on haproxy-2..."
    ssh rocky@haproxy-2 "sudo systemctl stop haproxy"
fi

# Wait for failover
echo "Waiting for HAProxy failover..."
sleep 15

# Test access after failover
if curl -s -I http://$MAIN_VIP:8081 | head -1; then
    echo "‚úÖ HAProxy failover successful - webserver still accessible"
else
    echo "‚ùå HAProxy failover failed"
fi

# Test database access through new HAProxy
export PGPASSWORD=airflow_pass
psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -c "SELECT 'Database via new HAProxy: SUCCESS';" && echo "‚úÖ Database via new HAProxy: OK"

# Restart stopped HAProxy
if [ "$PRIMARY" = "haproxy-1" ]; then
    sudo systemctl start haproxy
else
    ssh rocky@haproxy-2 "sudo systemctl start haproxy"
fi

echo "HAProxy failover test completed"
echo ""
```

**Test NFS HA Failover:**
```bash
echo "=== NFS HA FAILOVER TEST ==="

# CUSTOMIZE this value:
NFS_VIP="192.168.1.220"  # Replace with your NFS VIP

# Check current NFS primary
if ip addr show | grep -q $NFS_VIP; then
    echo "nfs-1 is current NFS primary"
    PRIMARY_NFS="nfs-1"
else
    echo "nfs-2 is current NFS primary"  
    PRIMARY_NFS="nfs-2"
fi

# Test NFS access before failover
ls /mnt/airflow-dags/ >/dev/null 2>&1 && echo "NFS accessible before failover"

# Create test file before failover
echo "Test content before failover $(date)" | sudo tee /srv/airflow/dags/failover_test.txt

# Stop NFS service on primary to trigger failover
if [ "$PRIMARY_NFS" = "nfs-1" ]; then
    echo "Stopping NFS on nfs-1..."
    sudo systemctl stop nfs-server
else
    echo "Stopping NFS on nfs-2..."
    ssh rocky@nfs-2 "sudo systemctl stop nfs-server"
fi

# Wait for failover
echo "Waiting for NFS failover..."
sleep 20

# Test access after failover
if ls /mnt/airflow-dags/ >/dev/null 2>&1; then
    echo "‚úÖ NFS failover successful - storage still accessible"
    
    # Test file is still accessible
    if ls /mnt/airflow-dags/failover_test.txt >/dev/null 2>&1; then
        echo "‚úÖ File synchronization working"
    else
        echo "‚ùå File synchronization failed"
    fi
else
    echo "‚ùå NFS failover failed"
fi

# Create test file after failover to test reverse sync
echo "Test content after failover $(date)" | sudo tee /mnt/airflow-dags/failover_test2.txt

# Restart stopped NFS service
if [ "$PRIMARY_NFS" = "nfs-1" ]; then
    sudo systemctl start nfs-server
else
    ssh rocky@nfs-2 "sudo systemctl start nfs-server"
fi

echo "NFS failover test completed"
echo ""
```

**Test RabbitMQ Cluster Resilience:**
```bash
echo "=== RABBITMQ CLUSTER RESILIENCE TEST ==="

# Stop one RabbitMQ node
echo "Stopping rabbit-2 to test cluster resilience..."
ssh rocky@rabbit-2 "sudo systemctl stop rabbitmq-server"

# Wait for cluster to detect failure
sleep 10

# Test remaining nodes using hostnames
echo "Testing remaining RabbitMQ nodes..."
curl -s -u airflow_user:airflow_pass http://rabbit-1:15672/api/overview >/dev/null && echo "‚úÖ rabbit-1 still accessible"
curl -s -u airflow_user:airflow_pass http://rabbit-3:15672/api/overview >/dev/null && echo "‚úÖ rabbit-3 still accessible"

# Test Airflow can still queue tasks
airflow dags trigger test_distributed_ha_setup && echo "‚úÖ Task queuing still works with reduced cluster"

# Restart stopped node
ssh rocky@rabbit-2 "sudo systemctl start rabbitmq-server"
sleep 15

# Verify node rejoined cluster
ssh rocky@rabbit-2 "sudo rabbitmqctl cluster_status | grep -q 'rabbit-1.*rabbit-2.*rabbit-3'" && echo "‚úÖ rabbit-2 rejoined cluster"

echo "RabbitMQ cluster resilience test completed"
echo ""
```

### Step 6.4: Scheduler HA Coordination Testing

**Test multi-scheduler coordination:**
```bash
echo "=== SCHEDULER HA COORDINATION TEST ==="

# CUSTOMIZE this value:
MAIN_VIP="192.168.1.210"  # Replace with your Main VIP

# Check both schedulers are running
systemctl is-active --quiet airflow-scheduler && echo "‚úÖ haproxy-1 scheduler running"
ssh rocky@scheduler-2 "systemctl is-active --quiet airflow-scheduler" && echo "‚úÖ scheduler-2 scheduler running"

# Check scheduler jobs in database
export PGPASSWORD=airflow_pass
echo "Active scheduler jobs:"
psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -c "
SELECT hostname, state, latest_heartbeat, job_type 
FROM job 
WHERE job_type = 'SchedulerJob' 
AND state = 'running' 
ORDER BY latest_heartbeat DESC;
"

# Test scheduler failover
echo "Testing scheduler failover..."
echo "Stopping haproxy-1 scheduler..."
sudo systemctl stop airflow-scheduler

# Wait and check scheduler-2 continues processing
sleep 30
ssh rocky@scheduler-2 "sudo journalctl -u airflow-scheduler --since '1 minute ago' | grep -i 'processing\|scheduled'" && echo "‚úÖ scheduler-2 scheduler processing continues"

# Restart haproxy-1 scheduler
sudo systemctl start airflow-scheduler

# Check both schedulers coordinate
sleep 30
ACTIVE_SCHEDULERS=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT count(*) FROM job WHERE job_type = 'SchedulerJob' AND state = 'running';" | xargs)
echo "Active schedulers: $ACTIVE_SCHEDULERS"
[ "$ACTIVE_SCHEDULERS" -eq 2 ] && echo "‚úÖ Multi-scheduler coordination working" || echo "‚ùå Scheduler coordination issue"

echo "Scheduler HA coordination test completed"
echo ""
```

### Step 6.5: Load Balancer Testing

**Test webserver load balancing:**
```bash
echo "=== WEBSERVER LOAD BALANCING TEST ==="

# CUSTOMIZE this value:
MAIN_VIP="192.168.1.210"  # Replace with your Main VIP

# Test load balancing distribution
echo "Testing load balancing across webservers..."
for i in {1..10}; do
    echo -n "Request $i: "
    curl -s -I http://$MAIN_VIP:8081 | head -1
    sleep 1
done

# Test webserver failover
echo "Testing webserver failover..."

# Stop haproxy-1 webserver
sudo systemctl stop airflow-webserver
sleep 10

# Test access still works (should route to haproxy-2)
if curl -s -I http://$MAIN_VIP:8081 | head -1; then
    echo "‚úÖ Webserver failover successful"
else
    echo "‚ùå Webserver failover failed"
fi

# Restart haproxy-1 webserver
sudo systemctl start airflow-webserver
sleep 10

# Test load balancing is restored
echo "Testing load balancing restoration..."
curl -s -I http://$MAIN_VIP:8081 | head -1 && echo "‚úÖ Load balancing restored"

echo "Webserver load balancing test completed"
echo ""
```

### Step 6.6: Performance and Stress Testing

**Test system under load:**
```bash
echo "=== PERFORMANCE STRESS TEST ==="

# Create stress test DAG
sudo tee /srv/airflow/dags/stress_test_dag.py << 'EOF'
from datetime import datetime, timedelta
from airflow.decorators import dag, task
import time
import random

@dag(
    dag_id='stress_test_dag',
    schedule_interval=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=5,
    default_args={
        'owner': 'admin',
        'retries': 0,
    },
    tags=['stress', 'test']
)
def stress_test_dag():
    """Stress test DAG for performance testing"""
    
    @task
    def cpu_intensive_task(task_id: str):
        """CPU intensive task"""
        import socket
        start_time = time.time()
        
        # Simulate CPU work
        result = 0
        for i in range(1000000):
            result += i * random.random()
        
        end_time = time.time()
        return {
            'task_id': task_id,
            'hostname': socket.gethostname(),
            'duration': end_time - start_time,
            'result_sample': result % 1000
        }
    
    @task
    def memory_intensive_task(task_id: str):
        """Memory intensive task"""
        import socket
        start_time = time.time()
        
        # Simulate memory usage
        data = []
        for i in range(100000):
            data.append(f"Data point {i} with random value {random.random()}")
        
        end_time = time.time()
        return {
            'task_id': task_id,
            'hostname': socket.gethostname(),
            'duration': end_time - start_time,
            'data_points': len(data)
        }
    
    @task
    def io_intensive_task(task_id: str):
        """I/O intensive task"""
        import socket
        import tempfile
        import os
        
        start_time = time.time()
        
        # Simulate I/O work
        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
            for i in range(10000):
                f.write(f"Line {i}: {random.random()}\n")
            temp_file = f.name
        
        # Read the file back
        with open(temp_file, 'r') as f:
            lines = f.readlines()
        
        os.unlink(temp_file)
        end_time = time.time()
        
        return {
            'task_id': task_id,
            'hostname': socket.gethostname(),
            'duration': end_time - start_time,
            'lines_processed': len(lines)
        }
    
    # Create multiple tasks for parallel execution
    cpu_tasks = [cpu_intensive_task(f"cpu_task_{i}") for i in range(1, 6)]
    memory_tasks = [memory_intensive_task(f"memory_task_{i}") for i in range(1, 4)]
    io_tasks = [io_intensive_task(f"io_task_{i}") for i in range(1, 4)]
    
    # Run all tasks in parallel
    cpu_tasks + memory_tasks + io_tasks

dag_instance = stress_test_dag()
EOF

# Wait for DAG to be parsed
sleep 15

# Trigger multiple stress test runs
echo "Triggering stress test DAGs..."
for i in {1..3}; do
    airflow dags trigger stress_test_dag
    sleep 5
done

# Monitor system during stress test
echo "Monitoring system performance during stress test..."
echo "CPU usage, memory usage, and task execution will be monitored for 2 minutes..."

# CUSTOMIZE this value:
MAIN_VIP="192.168.1.210"  # Replace with your Main VIP

# Monitor for 2 minutes
for i in {1..24}; do
    echo "Monitor cycle $i/24:"
    
    # Check active task instances
    export PGPASSWORD=airflow_pass
    ACTIVE_TASKS=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT count(*) FROM task_instance WHERE state = 'running';" | xargs)
    echo "  Active tasks: $ACTIVE_TASKS"
    
    # Check scheduler health
    systemctl is-active --quiet airflow-scheduler && echo "  ‚úÖ haproxy-1 scheduler healthy" || echo "  ‚ùå haproxy-1 scheduler issue"
    ssh rocky@scheduler-2 "systemctl is-active --quiet airflow-scheduler" 2>/dev/null && echo "  ‚úÖ scheduler-2 scheduler healthy" || echo "  ‚ùå scheduler-2 scheduler issue"
    
    # Check worker health  
    ssh rocky@celery-1 "systemctl is-active --quiet airflow-worker" 2>/dev/null && echo "  ‚úÖ Worker healthy" || echo "  ‚ùå Worker issue"
    
    sleep 5
done

echo "Stress test monitoring completed"
echo ""
```

### Step 6.7: Monitoring and Alerting Setup

**Create monitoring dashboard:**
```bash
# Create monitoring script for continuous health checking
sudo tee /usr/local/bin/airflow-monitor.sh << 'EOF'
#!/bin/bash

LOG_FILE="/var/log/airflow-monitor.log"
ALERT_LOG="/var/log/airflow-alerts.log"

log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" >> $LOG_FILE
}

send_alert() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - ALERT: $1" >> $ALERT_LOG
    # Here you could add email notifications, Slack alerts, etc.
    logger "AIRFLOW ALERT: $1"
}

# Check VIPs - REPLACE_WITH_YOUR_VIPs
if ! ping -c 1 <MAIN_VIP> >/dev/null 2>&1; then
    send_alert "Main VIP (<MAIN_VIP>) not accessible"
fi

if ! ping -c 1 <NFS_VIP> >/dev/null 2>&1; then
    send_alert "NFS VIP (<NFS_VIP>) not accessible"
fi

# Check database
export PGPASSWORD=airflow_pass
if ! psql -h <MAIN_VIP> -U airflow_user -p 5000 -d airflow_db -c "SELECT 1;" >/dev/null 2>&1; then
    send_alert "PostgreSQL database not accessible via VIP"
fi

# Check RabbitMQ cluster - REPLACE_WITH_YOUR_RABBIT_IPs
HEALTHY_MQ=0
for node in <RABBIT_1_IP> <RABBIT_2_IP> <RABBIT_3_IP>; do
    if curl -s -u airflow_user:airflow_pass http://$node:15672/api/overview >/dev/null 2>&1; then
        HEALTHY_MQ=$((HEALTHY_MQ + 1))
    fi
done

if [ $HEALTHY_MQ -lt 2 ]; then
    send_alert "RabbitMQ cluster unhealthy - only $HEALTHY_MQ/3 nodes available"
fi

# Check schedulers
ACTIVE_SCHEDULERS=$(psql -h <MAIN_VIP> -U airflow_user -p 5000 -d airflow_db -t -c "SELECT count(*) FROM job WHERE job_type = 'SchedulerJob' AND state = 'running' AND latest_heartbeat > NOW() - INTERVAL '2 minutes';" 2>/dev/null | xargs)

if [ "$ACTIVE_SCHEDULERS" -lt 1 ]; then
    send_alert "No active schedulers found"
elif [ "$ACTIVE_SCHEDULERS" -lt 2 ]; then
    send_alert "Only $ACTIVE_SCHEDULERS scheduler active (expected 2)"
fi

# Check webserver access
if ! curl -s -I http://<MAIN_VIP>:8081 >/dev/null 2>&1; then
    send_alert "Webserver not accessible via load balancer"
fi

# Check NFS access
if ! ls /mnt/airflow-dags >/dev/null 2>&1; then
    send_alert "NFS mount not accessible"
fi

log_message "Health check completed - Schedulers: $ACTIVE_SCHEDULERS, RabbitMQ: $HEALTHY_MQ/3"
EOF

sudo chmod +x /usr/local/bin/airflow-monitor.sh
```

**üîß Monitoring Script IP Replacement Helper:**
```bash
# Replace IP placeholders in monitoring script
# CUSTOMIZE these values:
MAIN_VIP="192.168.1.210"         # Replace with your Main VIP
NFS_VIP="192.168.1.220"          # Replace with your NFS VIP
RABBIT_1_IP="192.168.1.15"       # Replace with rabbit-1 IP
RABBIT_2_IP="192.168.1.16"       # Replace with rabbit-2 IP
RABBIT_3_IP="192.168.1.17"       # Replace with rabbit-3 IP

# Apply replacements to monitoring script
sudo sed -i "s/<MAIN_VIP>/$MAIN_VIP/g" /usr/local/bin/airflow-monitor.sh
sudo sed -i "s/<NFS_VIP>/$NFS_VIP/g" /usr/local/bin/airflow-monitor.sh
sudo sed -i "s/<RABBIT_1_IP>/$RABBIT_1_IP/g" /usr/local/bin/airflow-monitor.sh
sudo sed -i "s/<RABBIT_2_IP>/$RABBIT_2_IP/g" /usr/local/bin/airflow-monitor.sh
sudo sed -i "s/<RABBIT_3_IP>/$RABBIT_3_IP/g" /usr/local/bin/airflow-monitor.sh

echo "Monitoring script updated with your IP addresses"

# Set up cron job for monitoring
echo "*/2 * * * * /usr/local/bin/airflow-monitor.sh" | sudo crontab -

# Create log rotation for monitoring logs
sudo tee /etc/logrotate.d/airflow-monitoring << EOF
/var/log/airflow-monitor.log /var/log/airflow-alerts.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 rocky rocky
}
EOF

echo "Monitoring setup completed"
```

### Step 6.8: Backup and Recovery Procedures

**Create backup scripts:**
```bash
# Create comprehensive backup script
sudo tee /usr/local/bin/airflow-backup.sh << 'EOF'
#!/bin/bash

BACKUP_DIR="/opt/airflow-backups"
DATE=$(date +%Y%m%d_%H%M%S)
LOG_FILE="/var/log/airflow-backup.log"

log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $LOG_FILE
}

# Create backup directory
sudo mkdir -p $BACKUP_DIR
cd $BACKUP_DIR

log_message "Starting Airflow HA backup process"

# 1. Database backup - REPLACE_WITH_YOUR_MAIN_VIP
log_message "Backing up PostgreSQL database..."
export PGPASSWORD=airflow_pass
if pg_dump -h <MAIN_VIP> -U airflow_user -p 5000 -d airflow_db > airflow_db_backup_$DATE.sql; then
    log_message "Database backup completed: airflow_db_backup_$DATE.sql"
else
    log_message "ERROR: Database backup failed"
fi

# 2. DAGs backup
log_message "Backing up DAGs..."
if tar -czf dags_backup_$DATE.tar.gz -C /srv/airflow dags; then
    log_message "DAGs backup completed: dags_backup_$DATE.tar.gz"
else
    log_message "ERROR: DAGs backup failed"
fi

# 3. Configuration backup
log_message "Backing up configurations..."
tar -czf config_backup_$DATE.tar.gz \
    /home/rocky/airflow/airflow.cfg \
    /etc/systemd/system/airflow-*.service \
    /etc/haproxy/haproxy.cfg \
    /etc/keepalived/keepalived.conf \
    /usr/patroni/conf/patroni.yml \
    /etc/rabbitmq/rabbitmq.conf 2>/dev/null

if [ $? -eq 0 ]; then
    log_message "Configuration backup completed: config_backup_$DATE.tar.gz"
else
    log_message "WARNING: Some configuration files may not have been backed up"
fi

# 4. SSL certificates backup
log_message "Backing up SSL certificates..."
tar -czf ssl_backup_$DATE.tar.gz /usr/patroni/conf/server.* 2>/dev/null
log_message "SSL certificates backup completed: ssl_backup_$DATE.tar.gz"

# 5. Cleanup old backups (keep 7 days)
log_message "Cleaning up old backups..."
find $BACKUP_DIR -type f -mtime +7 -delete
REMAINING_BACKUPS=$(ls -1 $BACKUP_DIR | wc -l)
log_message "Cleanup completed. Remaining backup files: $REMAINING_BACKUPS"

# 6. Backup verification
log_message "Verifying backup integrity..."
for backup_file in airflow_db_backup_$DATE.sql dags_backup_$DATE.tar.gz config_backup_$DATE.tar.gz; do
    if [ -f "$backup_file" ] && [ -s "$backup_file" ]; then
        SIZE=$(du -h "$backup_file" | cut -f1)
        log_message "‚úÖ $backup_file verified (Size: $SIZE)"
    else
        log_message "‚ùå $backup_file verification failed"
    fi
done

log_message "Backup process completed successfully"
echo "Backup location: $BACKUP_DIR"
echo "Database backup: airflow_db_backup_$DATE.sql"
echo "DAGs backup: dags_backup_$DATE.tar.gz"
echo "Config backup: config_backup_$DATE.tar.gz"
EOF

sudo chmod +x /usr/local/bin/airflow-backup.sh
```

**üîß Backup Script VIP Replacement Helper:**
```bash
# Replace VIP placeholder in backup script
# CUSTOMIZE this value:
MAIN_VIP="192.168.1.210"  # Replace with your Main VIP

# Apply replacement to backup script
sudo sed -i "s/<MAIN_VIP>/$MAIN_VIP/g" /usr/local/bin/airflow-backup.sh

echo "Backup script updated with Main VIP: $MAIN_VIP"

# Schedule daily backups
echo "0 2 * * * /usr/local/bin/airflow-backup.sh" | sudo crontab -

# Test backup script
echo "Testing backup script..."
sudo /usr/local/bin/airflow-backup.sh
```

### Step 6.9: Final System Validation

**Run comprehensive final validation:**
```bash
echo "=== FINAL SYSTEM VALIDATION ==="

# Run status check
sudo /usr/local/bin/airflow-ha-status.sh

# Test all access URLs - REPLACE_WITH_YOUR_ADDRESSES
echo ""
echo "=== ACCESS URL VALIDATION ==="

# CUSTOMIZE these values:
MAIN_VIP="192.168.1.210"         # Replace with your Main VIP
HAPROXY_1_IP="192.168.1.10"      # Replace with haproxy-1 IP
RABBIT_1_IP="192.168.1.15"       # Replace with rabbit-1 IP

curl -s -I http://$MAIN_VIP:8081 | head -1 && echo "‚úÖ Main Airflow UI accessible"
curl -s -I http://$HAPROXY_1_IP:5555 | head -1 && echo "‚úÖ Flower accessible"
curl -s -I http://$MAIN_VIP:7000 | head -1 && echo "‚úÖ HAProxy stats accessible"
curl -s -u airflow_user:airflow_pass http://$RABBIT_1_IP:15672/api/overview >/dev/null && echo "‚úÖ RabbitMQ management accessible"

# Test DAG execution end-to-end
echo ""
echo "=== END-TO-END DAG EXECUTION TEST ==="
airflow dags trigger test_distributed_ha_setup
echo "Waiting for DAG execution..."
sleep 90

# Check execution results
export PGPASSWORD=airflow_pass
LAST_RUN_STATE=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT state FROM dag_run WHERE dag_id = 'test_distributed_ha_setup' ORDER BY execution_date DESC LIMIT 1;" | xargs)

if [ "$LAST_RUN_STATE" = "success" ]; then
    echo "‚úÖ End-to-end DAG execution: SUCCESS"
else
    echo "‚ùå End-to-end DAG execution: $LAST_RUN_STATE"
fi

# Performance metrics
echo ""
echo "=== PERFORMANCE METRICS ==="
ACTIVE_SCHEDULERS=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT count(*) FROM job WHERE job_type = 'SchedulerJob' AND state = 'running';" | xargs)
TOTAL_DAGS=$(airflow dags list | wc -l)
RECENT_TASKS=$(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT count(*) FROM task_instance WHERE start_date > NOW() - INTERVAL '1 hour';" | xargs)

echo "Active Schedulers: $ACTIVE_SCHEDULERS"
echo "Total DAGs: $TOTAL_DAGS"
echo "Tasks executed (last hour): $RECENT_TASKS"

# Security validation
echo ""
echo "=== SECURITY VALIDATION ==="
echo "Database connections use SSL: $(psql -h $MAIN_VIP -U airflow_user -p 5000 -d airflow_db -t -c "SELECT ssl FROM pg_stat_ssl WHERE pid = pg_backend_pid();" | xargs)"
echo "Airflow authentication enabled: $(grep -q "auth_backends" ~/airflow/airflow.cfg && echo "YES" || echo "NO")"
echo "RabbitMQ authentication configured: $(sudo rabbitmqctl list_users | grep -q airflow_user && echo "YES" || echo "NO")"

echo ""
echo "=== SYSTEM VALIDATION COMPLETE ==="
```

### Step 6.10: Production Readiness Checklist

**Final production readiness verification:**
```bash
# CUSTOMIZE these values:
MAIN_VIP="192.168.1.210"         # Replace with your Main VIP
NFS_VIP="192.168.1.220"          # Replace with your NFS VIP
HAPROXY_1_IP="192.168.1.10"      # Replace with haproxy-1 IP
RABBIT_1_IP="192.168.1.15"       # Replace with rabbit-1 IP

echo "=== PRODUCTION READINESS CHECKLIST ==="

# Create final checklist
cat << EOF
AIRFLOW HA PRODUCTION READINESS CHECKLIST
=========================================

INFRASTRUCTURE COMPONENTS:
[ ] ‚úÖ PostgreSQL HA Cluster (3 nodes with Patroni)
[ ] ‚úÖ RabbitMQ HA Cluster (3 nodes with mirroring)
[ ] ‚úÖ NFS HA Storage (Active/Passive with VIP)
[ ] ‚úÖ HAProxy Load Balancer HA (Active/Passive with VIP)

AIRFLOW COMPONENTS:
[ ] ‚úÖ Scheduler HA (haproxy-1 + scheduler-2)
[ ] ‚úÖ Webserver HA (haproxy-1 + haproxy-2 load balanced)
[ ] ‚úÖ Worker Nodes (celery-1 with horizontal scaling)
[ ] ‚úÖ DAG Processor HA (nfs-1 + nfs-2)
[ ] ‚úÖ Flower Monitoring (haproxy-1)

HIGH AVAILABILITY FEATURES:
[ ] ‚úÖ Virtual IPs configured ($MAIN_VIP, $NFS_VIP)
[ ] ‚úÖ Automatic failover for all components
[ ] ‚úÖ Health monitoring and alerts
[ ] ‚úÖ Load balancing and distribution
[ ] ‚úÖ Data replication and synchronization

SECURITY:
[ ] ‚úÖ Database SSL encryption
[ ] ‚úÖ User authentication configured
[ ] ‚úÖ Service isolation via firewall
[ ] ‚úÖ SSH key-based authentication

MONITORING & MAINTENANCE:
[ ] ‚úÖ Health check scripts deployed
[ ] ‚úÖ Automated monitoring (cron jobs)
[ ] ‚úÖ Backup procedures configured
[ ] ‚úÖ Log rotation configured
[ ] ‚úÖ Alert mechanisms in place

TESTING COMPLETED:
[ ] ‚úÖ Database failover tested
[ ] ‚úÖ HAProxy failover tested  
[ ] ‚úÖ NFS failover tested
[ ] ‚úÖ RabbitMQ resilience tested
[ ] ‚úÖ Scheduler coordination tested
[ ] ‚úÖ Webserver load balancing tested
[ ] ‚úÖ End-to-end DAG execution tested
[ ] ‚úÖ Performance stress testing completed

ACCESS URLS:
- Main UI: http://$MAIN_VIP:8081
- Flower:  http://$HAPROXY_1_IP:5555
- HAProxy: http://$MAIN_VIP:7000
- RabbitMQ: http://$RABBIT_1_IP:15672

CREDENTIALS:
- Airflow UI: admin / admin123
- RabbitMQ: airflow_user / airflow_pass
- Database: airflow_user / airflow_pass

OPERATIONAL PROCEDURES:
‚úÖ Backup script: /usr/local/bin/airflow-backup.sh
‚úÖ Status check: /usr/local/bin/airflow-ha-status.sh
‚úÖ Health monitor: /usr/local/bin/airflow-monitor.sh
‚úÖ Daily backups scheduled (2 AM)
‚úÖ Health monitoring every 2 minutes

SYSTEM READY FOR PRODUCTION: YES ‚úÖ

EOF

echo "System validation and testing complete!"
echo ""
echo "Your enterprise-grade, highly available Apache Airflow infrastructure is now ready for production use."
echo ""
echo "Key achievements:"
echo "- Zero single points of failure"
echo "- Automatic failover for all components"
echo "- Load balancing and high performance"
echo "- Comprehensive monitoring and alerting"
echo "- Automated backup and recovery"
echo "- Production-ready security configuration"
```

**üîß Complete Final Validation Script:**
```bash
# Create comprehensive final validation script
# CUSTOMIZE these values:
MAIN_VIP="192.168.1.210"         # Replace with your Main VIP
NFS_VIP="192.168.1.220"          # Replace with your NFS VIP
HAPROXY_1_IP="192.168.1.10"      # Replace with haproxy-1 IP
RABBIT_1_IP="192.168.1.15"       # Replace with rabbit-1 IP

sudo tee /usr/local/bin/final_validation.sh << EOF
#!/bin/bash
MAIN_VIP="$MAIN_VIP"
NFS_VIP="$NFS_VIP"
HAPROXY_1_IP="$HAPROXY_1_IP"
RABBIT_1_IP="$RABBIT_1_IP"

echo "=== FINAL AIRFLOW HA VALIDATION ==="
echo "Main VIP: \$MAIN_VIP"
echo "NFS VIP: \$NFS_VIP"
echo ""

# Test all critical components
echo "Testing VIP accessibility..."
ping -c 2 \$MAIN_VIP >/dev/null 2>&1 && echo "‚úÖ Main VIP accessible" || echo "‚ùå Main VIP failed"
ping -c 2 \$NFS_VIP >/dev/null 2>&1 && echo "‚úÖ NFS VIP accessible" || echo "‚ùå NFS VIP failed"

echo "Testing web interfaces..."
curl -s -I http://\$MAIN_VIP:8081 >/dev/null 2>&1 && echo "‚úÖ Airflow UI accessible" || echo "‚ùå Airflow UI failed"
curl -s -I http://\$HAPROXY_1_IP:5555 >/dev/null 2>&1 && echo "‚úÖ Flower accessible" || echo "‚ùå Flower failed"

echo "Testing database..."
export PGPASSWORD=airflow_pass
psql -h \$MAIN_VIP -U airflow_user -p 5000 -d airflow_db -c "SELECT 1;" >/dev/null 2>&1 && echo "‚úÖ Database accessible" || echo "‚ùå Database failed"

echo "Testing message queue..."
curl -s -u airflow_user:airflow_pass http://\$RABBIT_1_IP:15672/api/overview >/dev/null 2>&1 && echo "‚úÖ RabbitMQ accessible" || echo "‚ùå RabbitMQ failed"

echo ""
echo "=== VALIDATION COMPLETE ==="
echo "Infrastructure is ready for production workloads"
EOF

sudo chmod +x /usr/local/bin/final_validation.sh
# Run after customizing: sudo /usr/local/bin/final_validation.sh
```

This completes the comprehensive system integration and testing phase. The distributed HA Airflow infrastructure has been thoroughly tested and validated for:

‚úÖ **Component Integration**: All services working together seamlessly  
‚úÖ **High Availability**: Automatic failover tested for all components  
‚úÖ **Performance**: System validated under stress conditions  
‚úÖ **Monitoring**: Continuous health monitoring and alerting configured  
‚úÖ **Backup/Recovery**: Automated backup procedures implemented  
‚úÖ **Production Readiness**: Complete validation checklist confirmed  

The infrastructure is now enterprise-ready with zero single points of failure, automatic failover capabilities, and comprehensive operational procedures for production deployment.

**üöÄ Your Enterprise-Grade Distributed HA Apache Airflow Infrastructure is Complete!**

- **13 VMs** coordinating in perfect harmony
- **Zero single points of failure** across all components
- **Automatic failover** for databases, load balancers, storage, and services
- **Production-ready monitoring** and alerting
- **Comprehensive backup and recovery** procedures
- **Load balancing** and horizontal scaling capabilities

The system is ready to handle enterprise workloads with confidence and reliability.

