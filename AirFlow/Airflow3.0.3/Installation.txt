Here's a comprehensive step-by-step guide for installing Apache Airflow 3.0.3 with RabbitMQ, Celery workers, PostgreSQL, and Rocky Linux 9 in a distributed HA implementation, including DAG distribution via NFS, FTP, and SSH targets:
#####################################################################################################
### System Architecture
```mermaid
graph TD
    subgraph Load Balancer
        LB[HAProxy]
    end
    
    subgraph Airflow Cluster
        W1[Web Server 1]
        W2[Web Server 2]
        S1[Scheduler 1]
        S2[Scheduler 2]
        Worker1[Celery Worker 1]
        Worker2[Celery Worker 2]
    end
    
    subgraph Storage
        NFS[NFS Server]
        FTP[FTP Server]
    end
    
    subgraph Database
        P1[PostgreSQL Primary]
        P2[PostgreSQL Standby 1]
        P3[PostgreSQL Standby 2]
    end
    
    subgraph Queue
        RMQ1[RabbitMQ Node 1]
        RMQ2[RabbitMQ Node 2]
        RMQ3[RabbitMQ Node 3]
    end
    
    subgraph Targets
        T1[SSH Target 1]
        T2[SSH Target 2]
    end
    
    LB --> W1 & W2
    W1 & W2 & S1 & S2 & Worker1 & Worker2 --> NFS
    Worker1 & Worker2 --> FTP
    Worker1 & Worker2 --> T1 & T2
    W1 & W2 & S1 & S2 & Worker1 & Worker2 --> P1
    S1 & S2 --> RMQ1
    Worker1 & Worker2 --> RMQ1
```
#####################################################################################################
### Step 1: Prerequisites (All Nodes)
1. **Rocky Linux 9 Setup:**
   ```bash
   sudo dnf update -y
   sudo dnf install -y epel-release
   sudo dnf config-manager --set-enabled crb
   sudo hostnamectl set-hostname airflow-node1  # Set unique hostnames
   ```

2. **Firewall Configuration:**
   ```bash
   sudo firewall-cmd --permanent --add-port={8080/tcp,5432/tcp,5672/tcp,4369/tcp,25672/tcp,2049/tcp,111/tcp}
   sudo firewall-cmd --reload
   ```
#####################################################################################################
### Step 2: NFS Server Setup (Central Storage)
1. **Install NFS:**
   ```bash
   sudo dnf install -y nfs-utils
   sudo systemctl enable --now nfs-server rpcbind
   ```

2. **Create Shared Directories:**
   ```bash
   sudo mkdir -p /shared/airflow/{dags,logs,plugins,data}
   sudo useradd airflow
   sudo chown -R airflow:airflow /shared/airflow
   sudo chmod -R 775 /shared/airflow
   ```

3. **Configure Exports (`/etc/exports`):**
   ```bash
   sudo vim /etc/exports
   
   /shared/airflow 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check)
   ```

4. **Apply Configuration:**
   ```bash
   sudo exportfs -a
   sudo systemctl restart nfs-server
   ```
#####################################################################################################
### Step 3: FTP Server Setup
1. **Install vsftpd:**
   ```bash
   sudo dnf install -y vsftpd
   ```

2. **Configure vsftpd (`/etc/vsftpd/vsftpd.conf`):**
   ```ini
   anonymous_enable=NO
   local_enable=YES
   write_enable=YES
   local_umask=022
   chroot_local_user=YES
   pasv_min_port=64000
   pasv_max_port=64321
   ```

3. **Create FTP User:**
   ```bash
   sudo useradd -m -d /shared/ftp ftp_user
   sudo passwd ftp_user
   sudo chown -R ftp_user:ftp_user /shared/ftp
   ```

4. **Start Service:**
   ```bash
   sudo systemctl enable --now vsftpd
   ```
#####################################################################################################
### Step 4: PostgreSQL HA Cluster (3 Nodes)
1. **Install PostgreSQL 15:**
   ```bash
   sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-9-x86_64/pgdg-redhat-repo-latest.noarch.rpm
   sudo dnf -qy module disable postgresql
   sudo dnf install -y postgresql15-server postgresql15-contrib repmgr_15
   ```

2. **Initialize Primary:**
   ```bash
   sudo /usr/pgsql-15/bin/postgresql-15-setup initdb
   sudo systemctl enable --now postgresql-15
   ```

3. **Configure PostgreSQL (`/var/lib/pgsql/15/data/postgresql.conf`):**
   ```ini
   listen_addresses = '*'
   shared_preload_libraries = 'repmgr'
   wal_level = replica
   max_wal_senders = 10
   max_replication_slots = 10
   ```

4. **Configure Access (`/var/lib/pgsql/15/data/pg_hba.conf`):**
   ```conf
   host    all             all             192.168.1.0/24        md5
   host    replication     repmgr          192.168.1.0/24        trust
   ```

5. **Create repmgr User:**
   ```bash
   sudo -u postgres psql -c "CREATE USER repmgr WITH SUPERUSER PASSWORD 'repmgrpass';"
   sudo -u postgres psql -c "CREATE DATABASE repmgr OWNER repmgr;"
   ```

6. **Register Primary:**
   ```bash
   sudo -u repmgr repmgr -d repmgr -U repmgr -f /etc/repmgr/15/repmgr.conf primary register
   ```

7. **Configure Standbys:**
   ```bash
   sudo -u repmgr repmgr -h primary-ip -U repmgr -d repmgr standby clone
   sudo systemctl start postgresql-15
   sudo -u repmgr repmgr standby register
   ```
#####################################################################################################
### Step 5: RabbitMQ Cluster (3 Nodes)
1. **Install RabbitMQ:**
   ```bash
   sudo dnf install -y erlang rabbitmq-server
   sudo systemctl enable --now rabbitmq-server
   ```

2. **Set Erlang Cookie:**
   ```bash
   sudo echo "RABBITMQ_CLUSTER_COOKIE" > /var/lib/rabbitmq/.erlang.cookie
   sudo chmod 600 /var/lib/rabbitmq/.erlang.cookie
   ```

3. **Join Cluster:**
   ```bash
   sudo rabbitmqctl stop_app
   sudo rabbitmqctl reset
   sudo rabbitmqctl join_cluster rabbit@node1
   sudo rabbitmqctl start_app
   ```

4. **Configure HA:**
   ```bash
   sudo rabbitmqctl set_policy ha-all ".*" '{"ha-mode":"all", "ha-sync-mode":"automatic"}'
   sudo rabbitmqctl add_user airflow airflowpass
   sudo rabbitmqctl add_vhost airflow_vhost
   sudo rabbitmqctl set_permissions -p airflow_vhost airflow ".*" ".*" ".*"
   ```
#####################################################################################################
### Step 6: Airflow Installation (All Nodes)
1. **Install Dependencies:**
   ```bash
   sudo dnf install -y python3.9 python3.9-devel gcc openssl-devel libffi-devel openldap-devel
   sudo alternatives --set python /usr/bin/python3.9
   ```

2. **Create Virtual Environment:**
   ```bash
   sudo useradd -r -s /sbin/nologin -d /opt/airflow airflow
   sudo mkdir /opt/airflow
   sudo chown airflow:airflow /opt/airflow
   sudo -u airflow python3.9 -m venv /opt/airflow/venv
   ```

3. **Install Airflow:**
   ```bash
   sudo -u airflow /opt/airflow/venv/bin/pip install "apache-airflow[celery,rabbitmq,postgres,ssh,ftp]==3.0.3" \
   --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.3/constraints-3.9.txt"
   ```

4. **Mount NFS Share:**
   ```bash
   sudo mkdir -p /opt/airflow/shared
   echo "nfs-server:/shared/airflow /opt/airflow/shared nfs defaults 0 0" | sudo tee -a /etc/fstab
   sudo mount -a
   sudo chown -R airflow:airflow /opt/airflow/shared
   ```
#####################################################################################################
### Step 7: Airflow Configuration
1. **Configure `airflow.cfg`:**
   ```ini
   [core]
   executor = CeleryExecutor
   dags_folder = /opt/airflow/shared/dags
   plugins_folder = /opt/airflow/shared/plugins
   sql_alchemy_conn = postgresql+psycopg2://airflow:airflowpass@postgres-primary/airflow
   parallelism = 32
   
   [logging]
   base_log_folder = /opt/airflow/shared/logs
   
   [celery]
   broker_url = amqp://airflow:airflowpass@rabbitmq-vip/airflow_vhost
   result_backend = db+postgresql://airflow:airflowpass@postgres-primary/airflow
   
   [scheduler]
   scheduler_ha = True
   max_threads = 4
   
   [webserver]
   base_url = http://airflow-lb:8080
   ```

2. **Initialize Database (Primary Node Only):**
   ```bash
   sudo -u airflow /opt/airflow/venv/bin/airflow db init
   sudo -u airflow /opt/airflow/venv/bin/airflow users create \
       --username admin \
       --firstname Admin \
       --lastname User \
       --role Admin \
       --email admin@example.com \
       --password adminpass
   ```
#####################################################################################################
### Step 8: Systemd Services
1. **Web Service (`/etc/systemd/system/airflow-web.service`):**
   ```ini
   [Unit]
   Description=Airflow webserver
   After=network.target
   
   [Service]
   User=airflow
   Group=airflow
   Environment="PATH=/opt/airflow/venv/bin"
   ExecStart=/opt/airflow/venv/bin/airflow webserver
   Restart=on-failure
   RestartSec=5s
   
   [Install]
   WantedBy=multi-user.target
   ```

2. **Scheduler Service (`/etc/systemd/system/airflow-scheduler.service`):**
   ```ini
   [Unit]
   Description=Airflow scheduler
   After=network.target
   
   [Service]
   User=airflow
   Group=airflow
   Environment="PATH=/opt/airflow/venv/bin"
   ExecStart=/opt/airflow/venv/bin/airflow scheduler
   Restart=on-failure
   RestartSec=5s
   
   [Install]
   WantedBy=multi-user.target
   ```

3. **Worker Service (`/etc/systemd/system/airflow-worker.service`):**
   ```ini
   [Unit]
   Description=Airflow celery worker
   After=network.target
   
   [Service]
   User=airflow
   Group=airflow
   Environment="PATH=/opt/airflow/venv/bin"
   ExecStart=/opt/airflow/venv/bin/airflow celery worker
   Restart=on-failure
   RestartSec=5s
   
   [Install]
   WantedBy=multi-user.target
   ```

4. **Enable Services:**
   ```bash
   sudo systemctl daemon-reload
   sudo systemctl enable --now airflow-web airflow-scheduler airflow-worker
   ```
#####################################################################################################
### Step 9: SSH Configuration for Target Systems
1. **Generate SSH Keys:**
   ```bash
   sudo -u airflow ssh-keygen -t rsa -b 4096 -f /opt/airflow/.ssh/id_rsa -q -N ""
   ```

2. **Distribute Public Key:**
   ```bash
   sudo -u airflow ssh-copy-id -i /opt/airflow/.ssh/id_rsa.pub user@target1
   sudo -u airflow ssh-copy-id -i /opt/airflow/.ssh/id_rsa.pub user@target2
   ```

3. **Test Connection:**
   ```bash
   sudo -u airflow ssh user@target1 "echo 'SSH successful'"
   ```
#####################################################################################################
### Step 10: Airflow Connections Setup
1. **FTP Connection:**
   ```bash
   sudo -u airflow /opt/airflow/venv/bin/airflow connections add \
       --conn-type ftp \
       --conn-host ftp-server \
       --conn-login ftp_user \
       --conn-password ftp_pass \
       ftp_default
   ```

2. **SSH Connection:**
   ```bash
   sudo -u airflow /opt/airflow/venv/bin/airflow connections add \
       --conn-type ssh \
       --conn-host target1 \
       --conn-login user \
       --conn-extra '{"key_file": "/opt/airflow/.ssh/id_rsa"}' \
       target1_ssh
   ```
#####################################################################################################
### Step 11: HAProxy Configuration (Load Balancer)
1. **Install HAProxy:**
   ```bash
   sudo dnf install -y haproxy
   ```

2. **Configure (`/etc/haproxy/haproxy.cfg`):**
   ```conf
   frontend airflow_http
       bind *:8080
       option forwardfor
       default_backend airflow_web
   
   backend airflow_web
       balance roundrobin
       option httpchk GET /health
       server web1 192.168.1.10:8080 check
       server web2 192.168.1.11:8080 check
   ```

3. **Start Service:**
   ```bash
   sudo systemctl enable --now haproxy
   ```
#####################################################################################################
### Step 12: Verification and Testing
1. **Check Services:**
   ```bash
   airflow db check
   rabbitmqctl list_queues -p airflow_vhost
   repmgr cluster show
   ```

2. **Test DAG Execution:**
   ```python
   from airflow import DAG
   from airflow.providers.ssh.operators.ssh import SSHOperator
   from airflow.providers.ftp.operators.ftp import FTPFileTransmitOperator
   from datetime import datetime
   
   with DAG('ha_test', schedule_interval='@daily', start_date=datetime(2023, 1, 1)) as dag:
       ftp_download = FTPFileTransmitOperator(
           task_id='download_data',
           ftp_conn_id='ftp_default',
           local_filepath='/opt/airflow/shared/data/input.txt',
           remote_filepath='/incoming/input.txt',
           operation='get'
       )
       
       process_task = SSHOperator(
           task_id='process_data',
           ssh_conn_id='target1_ssh',
           command='process_script.sh /data/input.txt /data/output.txt'
       )
       
       ftp_upload = FTPFileTransmitOperator(
           task_id='upload_results',
           ftp_conn_id='ftp_default',
           local_filepath='/opt/airflow/shared/data/output.txt',
           remote_filepath='/outgoing/output.txt',
           operation='put'
       )
       
       ftp_download >> process_task >> ftp_upload
   ```
#####################################################################################################
### High Availability Features:
1. **PostgreSQL:**
   - Automatic failover with repmgr
   - Streaming replication
   - Manual promotion: `repmgr standby promote`

2. **RabbitMQ:**
   - Mirrored queues across all nodes
   - Message persistence
   - Cluster-aware clients

3. **Airflow:**
   - Multiple schedulers with leader election
   - Stateless web servers behind load balancer
   - Distributed Celery workers

4. **NFS:**
   - Centralized DAG management
   - Shared logs for troubleshooting
   - Persistent storage for plugins

### Maintenance Tips:
1. **DAG Deployment:**
   ```bash
   rsync -avz dags/ nfs-server:/shared/airflow/dags/
   ```

2. **Log Monitoring:**
   ```bash
   tail -f /opt/airflow/shared/logs/*/*.log
   ```

3. **Cluster Health Checks:**
   ```bash
   # PostgreSQL
   repmgr cluster show
   
   # RabbitMQ
   rabbitmqctl cluster_status
   
   # Airflow
   airflow jobs check --job-type SchedulerJob
   ```

4. **Backup Strategy:**
   ```bash
   # PostgreSQL
   pg_dumpall -U airflow -h postgres-primary > airflow_backup.sql
   
   # DAGs and plugins
   tar czvf airflow_dags_$(date +%Y%m%d).tar.gz /shared/airflow/dags /shared/airflow/plugins
   ```

This implementation provides a fully distributed, highly available Airflow environment with:
- Automatic failover for all critical components
- Centralized DAG management via NFS
- Secure file transfer capabilities with FTP
- Remote execution on target systems via SSH
- Load-balanced web interface
- Scalable worker infrastructure

All components maintain redundancy and can survive node failures while ensuring consistent access to DAGs and processing resources.