

# گزارش پیشرفت پروژه: پیاده‌سازی Apache Airflow برای خودکارسازی تسک‌های اجرایی

## 1. مقدمه

در راستای بهینه‌سازی و خودکارسازی اجرای تعداد زیادی از اسکریپت‌های تکراری و متوالی بر روی سرورهای ریموت، پروژه‌ای با هدف پیاده‌سازی و بهره‌برداری از **Apache Airflow** آغاز شد. با توجه به افزایش پیچیدگی تسک‌ها و تعداد بالای سرورهای هدف، خطر بروز خطای انسانی، تاخیر در اجرا و نبود پایش دقیق در فرآیندهای فعلی، استفاده از ابزارهای اتوماسیون پیشرفته ضروری به نظر می‌رسد.

## 2. دلایل انتخاب Airflow

**Apache Airflow** به عنوان یک ابزار قدرتمند برای تعریف، زمان‌بندی، اجرا و مانیتورینگ جریان‌های کاری (Workflows) انتخاب شد. دلایل انتخاب این ابزار شامل موارد زیر است:

* تعریف ساختارمند و گراف‌مانند (DAG) برای تسک‌ها
* قابلیت زمان‌بندی دقیق و انعطاف‌پذیر
* قابلیت اتصال به سرورهای ریموت برای اجرای دستورات
* پشتیبانی از اجراهای توزیع‌شده و مقیاس‌پذیر با استفاده از Celery
* واسط کاربری گرافیکی برای مانیتورینگ و بررسی خطاها
* سازگاری بالا با زبان پایتون و سایر ابزارهای موجود در اکوسیستم DevOps

## 3. انتخاب ابزارهای پشتیبان

* **PostgreSQL**: به عنوان Backend Database برای ذخیره‌سازی metadata مربوط به DAGها و تسک‌ها، به علت پایداری بالا و سازگاری کامل با Airflow
* **RabbitMQ**: به عنوان message broker برای صف‌بندی تسک‌ها و ارتباط میان کامپوننت‌ها در معماری توزیع‌شده
* **Celery**: برای اجرای توزیع‌شده‌ی تسک‌ها روی چندین worker به صورت مقیاس‌پذیر و با عملکرد بالا

## 4. معماری و نحوه استقرار

در فاز اولیه، از محیط مجازی‌سازی (VMware) برای راه‌اندازی نمونه‌ی تست استفاده شد. این محیط شامل:

* یک ماشین برای اجرای core اجزای Airflow (Scheduler، Webserver، Worker)
* دو ماشین به عنوان هدف اجرای اسکریپت‌ها (Remote Targets)

در آینده، با گسترش پروژه به محیط عملیاتی، زیرساخت به صورت **Distributed و High Availability (HA)** طراحی خواهد شد، به‌طوری که هر جزء روی سرور اختصاصی با منابع مناسب اجرا گردد.

## 5. مقایسه دو معماری Airflow

### مزایای پیاده‌سازی Airflow به‌صورت تک‌ماشینه‌ای (Single Machine Deployment)

* سادگی در راه‌اندازی و نگهداری
* مناسب برای تیم‌های کوچک یا پروژه‌های آزمایشی
* کاهش هزینه‌های سخت‌افزاری و زیرساخت
* عدم نیاز به سیستم‌های اشتراک فایل

### مزایای پیاده‌سازی Airflow به‌صورت توزیع‌شده (Distributed Architecture)

* مقیاس‌پذیری بالا
* تفکیک نقش‌ها و امنیت بالاتر
* استفاده از Triggerer و Task Deferral
* استقلال اجزا و قابلیت مانیتورینگ بهتر
* امکان استفاده از ابزارهای ابری مانند Kubernetes و Helm

## 6. ملاحظات سخت‌افزاری در محیط تست

راه‌اندازی محیط تست با منابع محدود شامل:

* 16GB RAM
* HDD با سرعت 7200RPM
* پردازنده معمولی

به دلیل سنگینی عملیات اجرا و لاگ‌گیری روی چند VM همزمان، نیاز به RAM و I/O دیسک بالاتر وجود دارد.
طراحی محیط تست به‌گونه‌ای بوده است که بتواند حداقل نیازهای یک سناریوی واقعی را پوشش دهد، اما برای محیط عملیاتی باید زیرساخت قوی‌تری تهیه گردد.

## 7. مفاهیم کلیدی آموخته‌شده و پیاده‌سازی‌شده

* **Scheduler**: زمان‌بندی اجرای تسک‌ها براساس DAGهای تعریف‌شده
* **Webserver**: ارائه UI جهت مشاهده وضعیت DAGها، لاگ‌ها و مانیتورینگ
* **Worker**: اجرای تسک‌ها در معماری Celery
* **Operators**: ابزارهای اجرای وظایف (مانند `BashOperator`، `PythonOperator`، `SSHOperator`)
* **Sensors**: بررسی وضعیت تا آماده شدن منابع یا رویدادهای خارجی
* **TaskFlow-decorated functions**: روشی نوین و تابع‌محور در تعریف DAGها
* **Connections**: تعریف کانکشن‌های امن مانند SSH برای اتصال به سرورهای ریموت

## 8. کارهای انجام‌شده در دو هفته اخیر

* راه‌اندازی اولیه‌ی محیط تست با اجزای لازم Airflow
* بررسی و تهیه منابع سخت‌افزاری مناسب برای تست
* نصب و پیکربندی PostgreSQL، RabbitMQ، Celery و وابستگی‌های دیگر
* طراحی و تست DAGهای ابتدایی برای اجرای تسک‌های ساده روی سرورهای ریموت
* تست اتصال امن به سرورهای ریموت (دو سرور به عنوان target فعلاً تعریف شده‌اند)
* مانیتورینگ اجرای تسک‌ها از طریق واسط کاربری Airflow و بررسی لاگ‌ها
* آشنایی با تنظیم زمان‌بندی تسک‌ها و اجرای وابسته به ترتیب و زمان

> DAGهای فعلی بیشتر در سطح تست اولیه و بررسی امکان‌سنجی بوده‌اند و با موفقیت اجرا شده‌اند.

## 9. برنامه‌های آینده

با توجه به حساسیت بالای محیط عملیاتی، به‌ویژه در سیستم‌های پرداخت بانکی که نیازمند پایداری، امنیت و دقت بسیار بالا در اجرای عملیات هستند، لازم است فاز تست به‌شکل جامع، ساختارمند و مرحله‌ای انجام شود.

### فاز 1: طراحی سناریوهای تست جامع

برخی از سناریوهای کلیدی عبارت‌اند از:

* **تست صحت اجرای تسک‌ها در شرایط عادی**
* **تست تسک‌های وابسته به شرایط خارجی (Event-based)**

  * استفاده از Sensors برای سنجش فایل، وضعیت سرور یا پاسخ API
* **تست در مواجهه با خطا (Failure Scenarios)**

  * قطع ارتباط شبکه، عدم دسترسی به فایل، خطا در اجرای اسکریپت
* **تست بار (Load Testing)**
* **تست تکرارپذیری و idempotency**

### فاز 2: تحلیل و ارتقاء زیرساخت تست

* ارتقاء سخت‌افزار (SSD، RAM بیشتر، جداسازی Worker و Scheduler)
* شبیه‌سازی ساختار نهایی محیط Production
* اضافه کردن سرورهای هدف با شرایط متنوع

### فاز 3: پیاده‌سازی مانیتورینگ پیشرفته

* راه‌اندازی **Grafana + Prometheus**
* پایش متریک‌هایی مانند:

  * مدت زمان اجرای تسک‌ها
  * تعداد تسک‌های موفق/ناموفق
  * وضعیت منابع سرورهای Worker
  * حجم لاگ‌ها و تعداد retryها
* تعریف Alert برای تسک‌های حیاتی

### فاز 4: بررسی امنیت و کنترل دسترسی

* تعیین Role و دسترسی کاربران
* جداسازی فایل‌ها و رمزهای اتصال
* رمزنگاری اتصالات و داده‌ها
* نگهداری امن رمزها و کلیدها

### فاز 5: آماده‌سازی برای معماری توزیع‌شده و عملیاتی

* استقرار اجزای Airflow به صورت جداگانه
* استفاده از message broker واقعی (RabbitMQ) در بستر پایدار

---

> گذر از مرحله تست سطحی به مرحله تست جدی و دقیق، نیازمند زمان، حوصله و همکاری نزدیک تیم‌های زیرساخت، امنیت و توسعه است.
> مطمئناً سرمایه‌گذاری زمانی و فنی در این مرحله، مانع بروز خطا در زمان اجرای واقعی خواهد شد و باعث افزایش اعتماد، پایداری و کیفیت در اجرای خودکار وظایف مهم عملیاتی خواهد بود.

